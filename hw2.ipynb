{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy.io\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    " \n",
    "def import_data(split = 0.8, shuffle=False):\n",
    "    \"\"\"Read in the data, split it by split percentage into train and test data, \n",
    "    and return X_train, y_train, X_test, y_test as numpy arrays\"\"\"\n",
    "    col_names = ['var', 'skew', 'curt', 'entropy', 'is_fraud']\n",
    "    \n",
    "    data = pd.read_csv('data_banknote.csv', names=col_names)\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.shuffle(data)\n",
    "\n",
    "    X_data = np.asarray(data[col_names[:-1]])\n",
    "    y_data = np.asarray(data['is_fraud'])\n",
    "\n",
    "    X_train = X_data[0:int(X_data.shape[0] * split)]\n",
    "    y_train = y_data[0:int(X_data.shape[0] * split)]\n",
    "    X_test = X_data[int(X_data.shape[0] * split):]\n",
    "    y_test = y_data[int(X_data.shape[0] * split):]\n",
    "\n",
    "\n",
    "    print(\"data imported\")\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "class Node:\n",
    "    \"\"\"Each node of decision tree will hold values such as left and right children, \n",
    "    the data and labels being split on, the threshold value & index in the dataframe for a particular feature,\n",
    "    and the uncertainty measure for this node\"\"\"\n",
    "    def __init__(self, data, labels, depth):\n",
    "        \"\"\"\n",
    "        data: X data\n",
    "        labels: y data\n",
    "        depth: depth of tree\n",
    "        \"\"\"\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.depth = depth\n",
    "\n",
    "        self.threshold = None\n",
    "        self.threshold_index = None\n",
    "        self.feature = None\n",
    "        self.label = None\n",
    "        self.uncertainty = None\n",
    "        \n",
    "class DecisionTree:\n",
    "    def __init__(self, K=5, verbose=False):\n",
    "        \"\"\"\n",
    "        K: number of features to split on \n",
    "        \"\"\"\n",
    "        self.root = None\n",
    "        self.K = K\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def buildTree(self, data, labels):\n",
    "        \"\"\"Builds tree for training on data. Recursively called _buildTree\"\"\"\n",
    "        self.root = Node(data, labels, 0)\n",
    "        if self.verbose:\n",
    "            print(\"Root node shape: \", data.shape, labels.shape)\n",
    "        self._buildTree(self.root)\n",
    "\n",
    "    def _buildTree(self, node):\n",
    "             \n",
    "        # get uncertainty measure and feature threshold\n",
    "        node.uncertainty = self.get_uncertainty(node.labels)\n",
    "        self.get_feature_threshold(node)\n",
    "            \n",
    "        index = node.data[:, node.feature].argsort()  # sort feature for return\n",
    "        node.data = node.data[index]\n",
    "        node.labels = node.labels[index]\n",
    "        \n",
    "        # check label distribution.\n",
    "        label_distribution = np.bincount(node.labels)\n",
    "        majority_label = node.labels[0] if len(label_distribution) == 1 else np.argmax(label_distribution)\n",
    "            \n",
    "        if self.verbose:\n",
    "            print(\"Node uncertainty: %f\" % node.uncertainty)\n",
    "    \n",
    "        # Split left and right if threshold is not the min or max of the feature or every point has the\n",
    "        # same label.\n",
    "        if node.threshold_index == 0 or node.threshold_index == node.data.shape[0] or \\\n",
    "            len(label_distribution) == 1:\n",
    "            node.label = majority_label\n",
    "        else:\n",
    "            node.left = Node(node.data[:node.threshold_index], node.labels[:node.threshold_index], node.depth + 1)\n",
    "            node.right = Node(node.data[node.threshold_index:], node.labels[node.threshold_index:], node.depth + 1)            \n",
    "            node.data = None\n",
    "            node.labels = None\n",
    "                        \n",
    "            # If in last layer of tree, assign predictions\n",
    "            if node.depth == self.K:\n",
    "                if len(node.left.labels) == 0:\n",
    "                    node.right.label = np.argmax(np.bincount(node.right.labels))\n",
    "                    node.left.label = 1 - node.right.label\n",
    "                elif len(node.right.labels) == 0:\n",
    "                    node.left.label = np.argmax(np.bincount(node.left.labels))\n",
    "                    node.right.label = 1 - node.left.label\n",
    "                else:\n",
    "                    node.left.label = np.argmax(np.bincount(node.left.labels))\n",
    "                    node.right.label = np.argmax(np.bincount(node.right.labels))\n",
    "                return\n",
    "\n",
    "            else: # Otherwise continue training the tree by calling _buildTree\n",
    "                self._buildTree(node.left)\n",
    "                self._buildTree(node.right)\n",
    "\n",
    "    def predict(self, data_pt):\n",
    "        return self._predict(data_pt, self.root)\n",
    "\n",
    "    def _predict(self, data_pt, node):\n",
    "        feature = node.feature\n",
    "        threshold = node.threshold\n",
    "\n",
    "        if node.label is not None:\n",
    "            return node.label\n",
    "        elif data_pt[node.feature] < node.threshold:\n",
    "            return self._predict(data_pt, node.left)\n",
    "        elif data_pt[node.feature] >= node.threshold:\n",
    "            return self._predict(data_pt, node.right)\n",
    "\n",
    "    def get_feature_threshold(self, node):\n",
    "        \"\"\" Find the feature that gives the largest information gain. Update node.threshold, \n",
    "        node.threshold_index, and node.feature (a number representing the feature. e.g. 2nd column feature would be 1)\n",
    "        \"\"\"\n",
    " \n",
    "        node.threshold = 0\n",
    "        node.threshold_index = 0\n",
    "        node.feature = 0\n",
    "        max_val = 0\n",
    "        \n",
    "        for i in range(node.data.shape[1]): # for each feature \n",
    "\n",
    "            index = node.data[:, i].argsort() # sort feature for return\n",
    "            node.data = node.data[index]\n",
    "            node.labels = node.labels[index]\n",
    "\n",
    "            _, index = np.unique(node.data[:, i], return_index=True)\n",
    "\n",
    "            for j in index: # for each unique pixel\n",
    "\n",
    "                value = self.getInfoGain(node, j)\n",
    "                if value > max_val: # and node.data[j, i] != node.data[0, i] and node.data[j, i] != node.data[-1, i]:\n",
    "                    max_val = value\n",
    "                    node.threshold = node.data[j, i]\n",
    "                    node.threshold_index = j\n",
    "                    node.feature = i\n",
    "            \n",
    "\n",
    "    def getInfoGain(self, node, split_index):\n",
    "        \"\"\"\n",
    "        Get information gain using the variables in the parameters, node.data, \n",
    "        node.labels, and node.uncertainty\n",
    "        split_index: index in the feature column that you are splitting the classes on\n",
    "        \"\"\"\n",
    " \n",
    "        return node.uncertainty - ((split_index / node.data.shape[0]) * self.get_uncertainty(node.labels[:split_index])\n",
    "                                   + ((node.data.shape[0] - split_index) / node.data.shape[0]) * self.get_uncertainty(node.labels[split_index:]))\n",
    "\n",
    "    def get_uncertainty(self, labels, metric=\"entropy\"):\n",
    "        \"\"\"Get uncertainty. Implement entropy, classification, and gini error. \n",
    "        np.bincount(labels) and labels.shape might be useful here\"\"\"\n",
    "        \n",
    "        if labels.shape[0] == 0:\n",
    "            return 1\n",
    " \n",
    "        if metric == \"classification\":\n",
    "            return 1 - np.max(np.bincount(labels) / labels.shape[0]) # classification error\n",
    "        elif metric == \"gini\":\n",
    "            return 1 - np.sum((np.bincount(labels) / labels.shape[0]) ** 2) # gini index\n",
    "        else:\n",
    "            count = np.bincount(labels)\n",
    "            if len(count) == 1:\n",
    "                return 0\n",
    "            else:                \n",
    "                if count[0] == 0 or count[0] == 0:\n",
    "                    return 0\n",
    "                else:\n",
    "                    return (- (count / labels.shape[0]) * np.log2(count / labels.shape[0]) ).sum() # entropy\n",
    "\n",
    "\n",
    "    def printTree(self):\n",
    "        \"\"\"Prints the tree including threshold value and feature name\"\"\"\n",
    "        self._printTree(self.root)\n",
    "\n",
    "    def _printTree(self, node):\n",
    "        if node is not None:\n",
    "            if node.label is None:\n",
    "                print(\"\\t\" * node.depth, \"(%d, %d)\" % (node.threshold, node.feature))\n",
    "            else:\n",
    "                print(\"\\t\" * node.depth, node.label)\n",
    "            self._printTree(node.left)\n",
    "            self._printTree(node.right)\n",
    "\n",
    "    def homework_evaluate(self, X_train, labels, X_test, y_test):\n",
    "        n = X_train.shape[0]\n",
    "\n",
    "        count = 0\n",
    "        for i in range(n):\n",
    "            if self.predict(X_train[i]) == labels[i]:\n",
    "                count += 1\n",
    "\n",
    "        print(\"The decision tree is %d percent accurate on %d training data\" % ((count / n) * 100, n))\n",
    "\n",
    "        n = X_test.shape[0]\n",
    "\n",
    "        count = 0\n",
    "        for i in range(n):\n",
    "            if self.predict(X_test[i]) == y_test[i]:\n",
    "                count += 1\n",
    "\n",
    "        print(\"The decision tree is %d percent accurate on %d test data\" % ((count / n) * 100, n))\n",
    "\n",
    "        return count / n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "data imported\n",
      "For K =  0\n",
      "The decision tree is 84 percent accurate on 1097 training data\n",
      "The decision tree is 88 percent accurate on 275 test data\n",
      "For K =  1\n",
      "The decision tree is 91 percent accurate on 1097 training data\n",
      "The decision tree is 82 percent accurate on 275 test data\n",
      "For K =  2\n",
      "The decision tree is 95 percent accurate on 1097 training data\n",
      "The decision tree is 96 percent accurate on 275 test data\n",
      "For K =  3\n",
      "The decision tree is 96 percent accurate on 1097 training data\n",
      "The decision tree is 95 percent accurate on 275 test data\n",
      "For K =  4\n",
      "The decision tree is 99 percent accurate on 1097 training data\n",
      "The decision tree is 96 percent accurate on 275 test data\n",
      "For K =  5\n",
      "The decision tree is 99 percent accurate on 1097 training data\n",
      "The decision tree is 96 percent accurate on 275 test data\n",
      "For K =  6\n",
      "The decision tree is 100 percent accurate on 1097 training data\n",
      "The decision tree is 96 percent accurate on 275 test data\n",
      "For K =  7\n",
      "The decision tree is 100 percent accurate on 1097 training data\n",
      "The decision tree is 96 percent accurate on 275 test data\n",
      "For K =  8\n",
      "The decision tree is 100 percent accurate on 1097 training data\n",
      "The decision tree is 96 percent accurate on 275 test data\n",
      "For K =  9\n",
      "The decision tree is 100 percent accurate on 1097 training data\n",
      "The decision tree is 96 percent accurate on 275 test data\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = import_data(split=0.8)\n",
    "\n",
    "for k_value in range(10):\n",
    "    print('For K = ', k_value)\n",
    "    tree = DecisionTree(K=k_value, verbose=False)\n",
    "    tree.buildTree(X_train, y_train)\n",
    "    tree.homework_evaluate(X_train, y_train, X_test, y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}